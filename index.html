<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="Bin&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Bin&#39;s Blog">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bin&#39;s Blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Bin's Blog</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?eb4b531447906baef3b3c138130db2ad";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Bin's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/21/04.2-Clustering-KMeans-zh/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="彬叔">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bin's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/21/04.2-Clustering-KMeans-zh/" itemprop="url">第六篇 聚类：深入K-Means</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-21T12:37:16+08:00">
                2017-11-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/11/21/04.2-Clustering-KMeans-zh/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/11/21/04.2-Clustering-KMeans-zh/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/11/21/04.2-Clustering-KMeans-zh/" class="leancloud_visitors" data-flag-title="第六篇 聚类：深入K-Means">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="聚类：深入K-Means"><a href="#聚类：深入K-Means" class="headerlink" title="聚类：深入K-Means"></a>聚类：深入K-Means</h1><p>这篇我们探讨一下<strong>K-Means聚类</strong>，它属于一种非监督聚类技术。</p>
<p>先导入一些标准方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</div><div class="line"></div><div class="line"><span class="comment"># use seaborn plotting defaults</span></div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns; sns.set()</div></pre></td></tr></table></figure>
<h2 id="介绍K-Means"><a href="#介绍K-Means" class="headerlink" title="介绍K-Means"></a>介绍K-Means</h2><p>K-Means是一种<strong>非监督聚类</strong>算法，它不是通过标签来聚类而是数据本身所拥有的属性。</p>
<p>K-Means是一种相对易于理解的算法。它搜索每个簇的中心点，把离中心点最近的点归为那个簇。</p>
<p>让我们看下K-Means如何来操作一个简单的簇群。强调一下，这是非监督算法，我们不会给簇上色。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</div><div class="line">X, y = make_blobs(n_samples=<span class="number">300</span>, centers=<span class="number">4</span>,</div><div class="line">                  random_state=<span class="number">0</span>, cluster_std=<span class="number">0.60</span>)</div><div class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], s=<span class="number">50</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-042507.jpg" alt="png"></p>
<p>用肉眼能相对容易找出这四簇。如果你执行全面的搜索来划分数据集，搜索空间将达到指数级。幸运的是，sklearn实现了一个著名的<em>Expectation Maximization (EM)</em>方法来帮我们解决这个问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</div><div class="line">est = KMeans(<span class="number">4</span>)  <span class="comment"># 4 clusters</span></div><div class="line">est.fit(X)</div><div class="line">y_kmeans = est.predict(X)</div><div class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_kmeans, s=<span class="number">50</span>, cmap=<span class="string">'rainbow'</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-043152.jpg" alt="png"></p>
<p>算法区分了4种我们能用肉眼看到的簇！</p>
<h2 id="K-Means算法：期望最大化（EM）"><a href="#K-Means算法：期望最大化（EM）" class="headerlink" title="K-Means算法：期望最大化（EM）"></a>K-Means算法：期望最大化（EM）</h2><p>K-Means使用了一种叫做<em>期望最大化（EM）</em>的算法来解决问题。<br><em>期望最大化（EM）</em>有下面的两个步骤：</p>
<ol>
<li>猜测一些簇心位置</li>
<li>重复直至收敛<ul>
<li>为每个点分配一个离他最近的簇心位置</li>
<li>用平均值法来重新计算簇心位置</li>
</ul>
</li>
</ol>
<p>让我们可视化这个过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> fig_code <span class="keyword">import</span> plot_kmeans_interactive</div><div class="line">plot_kmeans_interactive();</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-043211.jpg" alt="png"></p>
<p>算法会在合理的簇心下面得到收敛。</p>
<h2 id="K-Means需要注意的地方"><a href="#K-Means需要注意的地方" class="headerlink" title="K-Means需要注意的地方"></a>K-Means需要注意的地方</h2><p>这个算法不能保证收敛。所以sklearn默认会使用许多随机初始化的值从而发现最佳结果。</p>
<p>而且簇的数量事先得确定…</p>
<h2 id="K-Means在数字上的应用"><a href="#K-Means在数字上的应用" class="headerlink" title="K-Means在数字上的应用"></a>K-Means在数字上的应用</h2><p>一个更实际的例子，让我们再看一下数字。这里我们将使用k-means来聚类64维的数字，然后看下算法发现的簇心是什么？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line">digits = load_digits()</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">est = KMeans(n_clusters=<span class="number">10</span>)</div><div class="line">clusters = est.fit_predict(digits.data)</div><div class="line">est.cluster_centers_.shape</div></pre></td></tr></table></figure>
<pre><code>(10, 64)
</code></pre><p>我们看到10个簇。让我们可视化每一个簇心代表的数字：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">3</span>))</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    ax = fig.add_subplot(<span class="number">2</span>, <span class="number">5</span>, <span class="number">1</span> + i, xticks=[], yticks=[])</div><div class="line">    ax.imshow(est.cluster_centers_[i].reshape((<span class="number">8</span>, <span class="number">8</span>)), cmap=plt.cm.binary)</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-043225.jpg" alt="png"></p>
<p>K-Means能够发现这些簇，把它们的均值视作可以识别的数字！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> mode</div><div class="line"></div><div class="line">labels = np.zeros_like(clusters)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">    mask = (clusters == i)</div><div class="line">    labels[mask] = mode(digits.target[mask])[<span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>作为对比，让我们使用PCA可视化，并看下真实的簇标签和K-means簇标签：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line"></div><div class="line">X = PCA(<span class="number">2</span>).fit_transform(digits.data)</div><div class="line"></div><div class="line">kwargs = dict(cmap = plt.cm.get_cmap(<span class="string">'rainbow'</span>, <span class="number">10</span>),</div><div class="line">              edgecolor=<span class="string">'none'</span>, alpha=<span class="number">0.6</span>)</div><div class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">8</span>, <span class="number">4</span>))</div><div class="line">ax[<span class="number">0</span>].scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=labels, **kwargs)</div><div class="line">ax[<span class="number">0</span>].set_title(<span class="string">'learned cluster labels'</span>)</div><div class="line"></div><div class="line">ax[<span class="number">1</span>].scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=digits.target, **kwargs)</div><div class="line">ax[<span class="number">1</span>].set_title(<span class="string">'true labels'</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-043236.jpg" alt="png"></p>
<p>让我们看下K-Means分类器（无标签信息）的准确性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</div><div class="line">accuracy_score(digits.target, labels)</div></pre></td></tr></table></figure>
<pre><code>0.79298831385642743
</code></pre><p>接近80%，结果还行。我们看下这个的混淆矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</div><div class="line">print(confusion_matrix(digits.target, labels))</div><div class="line"></div><div class="line">plt.imshow(confusion_matrix(digits.target, labels),</div><div class="line">           cmap=<span class="string">'Blues'</span>, interpolation=<span class="string">'nearest'</span>)</div><div class="line">plt.colorbar()</div><div class="line">plt.grid(<span class="keyword">False</span>)</div><div class="line">plt.ylabel(<span class="string">'true'</span>)</div><div class="line">plt.xlabel(<span class="string">'predicted'</span>);</div></pre></td></tr></table></figure>
<pre><code>[[177   0   0   0   1   0   0   0   0   0]
 [  0  55  24   1   0   1   2   0  99   0]
 [  1   2 148  13   0   0   0   3   8   2]
 [  0   0   0 154   0   2   0   7   7  13]
 [  0   5   0   0 164   0   0   9   3   0]
 [  0   0   0   1   2 136   1   0   0  42]
 [  1   1   0   0   0   0 177   0   2   0]
 [  0   2   0   0   0   0   0 175   2   0]
 [  0   6   3   2   0   4   2   5 100  52]
 [  0  20   0   6   0   6   0   7   2 139]]
</code></pre><p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-043253.jpg" alt="png"></p>
<p>这是一个没有标签的<strong>完全无监督estimator</strong>，具有80%的分类准确率。</p>
<h2 id="例子：K-Means应用在图像的颜色空间压缩上"><a href="#例子：K-Means应用在图像的颜色空间压缩上" class="headerlink" title="例子：K-Means应用在图像的颜色空间压缩上"></a>例子：K-Means应用在图像的颜色空间压缩上</h2><p>一个有趣的应用是在图像的颜色空间压缩上。例如，想象你有一张上百万颜色像素的图片。在大多数图片中，颜色中的大部分不会被使用，而大部分的像素点颜色是相似甚至一样的。</p>
<p>sklearn内置了一些图片，可以通过加载模块来使用。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_sample_image</div><div class="line">china = load_sample_image(<span class="string">"china.jpg"</span>)</div><div class="line">plt.imshow(china)</div><div class="line">plt.grid(<span class="keyword">False</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-043307.jpg" alt="png"></p>
<p>图片本身包含了3维数组<code>(height, width, RGB)</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">china.shape</div></pre></td></tr></table></figure>
<pre><code>(427, 640, 3)
</code></pre><p>我们能想象这图片是3维颜色空间的点所组成。我们把像素点的取值范围限制在（0，1）的范围，再把位于一个平面上的像素点reshape成一条直线。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">X = (china / <span class="number">255.0</span>).reshape(<span class="number">-1</span>, <span class="number">3</span>)</div><div class="line">print(X.shape)</div></pre></td></tr></table></figure>
<pre><code>(273280, 3)
</code></pre><p>我们现在有273280个3维的像素点。</p>
<p>我们的任务是使用K-Means压缩$256^3$种颜色空间到一个很小的颜色空间（64种颜色空间）。首先我们要在数据集中发现$N_{color}$ 个簇，每个簇代表一个颜色，把属于一个簇的像素点用这个簇的颜色来替代。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># reduce the size of the image for speed</span></div><div class="line">image = china[::<span class="number">3</span>, ::<span class="number">3</span>]</div><div class="line">n_colors = <span class="number">64</span></div><div class="line"></div><div class="line">X = (image / <span class="number">255.0</span>).reshape(<span class="number">-1</span>, <span class="number">3</span>)</div><div class="line">    </div><div class="line">model = KMeans(n_colors)</div><div class="line">labels = model.fit_predict(X)</div><div class="line">colors = model.cluster_centers_</div><div class="line">new_image = colors[labels].reshape(image.shape)</div><div class="line">new_image = (<span class="number">255</span> * new_image).astype(np.uint8)</div><div class="line"></div><div class="line"><span class="comment"># create and plot the new image</span></div><div class="line"><span class="keyword">with</span> sns.axes_style(<span class="string">'white'</span>):</div><div class="line">    plt.figure()</div><div class="line">    plt.imshow(image)</div><div class="line">    plt.title(<span class="string">'input'</span>)</div><div class="line"></div><div class="line">    plt.figure()</div><div class="line">    plt.imshow(new_image)</div><div class="line">    plt.title(<span class="string">'&#123;0&#125; colors'</span>.format(n_colors))</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-043320.jpg" alt="png"></p>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-043349.jpg" alt="png"></p>
<p>比较输入和输出结果：我们把$256^3$种的颜色空间降到了64种</p>
<hr>
<p><a href="https://github.com/yummybian/sklearn_pycon2015_zh.git" target="_blank" rel="external">Jupyter实现</a></p>
<hr>
<p><a rel="external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/3.0/88x31.png"></a><br>本作品采用<a rel="external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank">知识共享署名-非商业性使用-禁止演绎 3.0 未本地化版本许可协议</a>进行许可。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/21/04.1-Dimensionality-PCA-zh/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="彬叔">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bin's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/21/04.1-Dimensionality-PCA-zh/" itemprop="url">第五篇 降维：深入主成分分析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-21T12:35:16+08:00">
                2017-11-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/11/21/04.1-Dimensionality-PCA-zh/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/11/21/04.1-Dimensionality-PCA-zh/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/11/21/04.1-Dimensionality-PCA-zh/" class="leancloud_visitors" data-flag-title="第五篇 降维：深入主成分分析">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="降维：深入主成分分析"><a href="#降维：深入主成分分析" class="headerlink" title="降维：深入主成分分析"></a>降维：深入主成分分析</h1><p>这里我们将探索<strong>主成分分析</strong>（PCA）, 这是非常有用的线性维度下降技术。</p>
<p>我们先导入一些标准模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, division</div><div class="line"></div><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</div><div class="line"></div><div class="line"><span class="comment"># use seaborn plotting style defaults</span></div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns; sns.set()</div></pre></td></tr></table></figure>
<h2 id="介绍主成分分析（Principal-components-analysis）"><a href="#介绍主成分分析（Principal-components-analysis）" class="headerlink" title="介绍主成分分析（Principal components analysis）"></a>介绍主成分分析（Principal components analysis）</h2><p>主成分分析是一种非常强大的用来对数据降维的非监督方法。最简单的是看下面一组二维数据的可视化过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">np.random.seed(<span class="number">1</span>)</div><div class="line">X = np.dot(np.random.random(size=(<span class="number">2</span>, <span class="number">2</span>)), np.random.normal(size=(<span class="number">2</span>, <span class="number">200</span>))).T</div><div class="line">plt.plot(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], <span class="string">'o'</span>)</div><div class="line">plt.axis(<span class="string">'equal'</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-041317.jpg" alt="png"></p>
<p>我们能看到在数据里面包含了确定的趋势。PCA能在数据中发现<strong>关键的成分</strong>，并能够描述那些成分在数据分布中的重要性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line"><span class="comment"># 特征维度数目</span></div><div class="line">pca = PCA(n_components=<span class="number">2</span>)</div><div class="line">pca.fit(X)</div><div class="line"><span class="comment"># 降维后的各主成分的方差值</span></div><div class="line">print(pca.explained_variance_)</div><div class="line">print(pca.components_)</div></pre></td></tr></table></figure>
<pre><code>[ 0.75871884  0.01838551]
[[-0.94446029 -0.32862557]
 [-0.32862557  0.94446029]]
</code></pre><p>看下这些值是什么意思，让我们把它们作为向量绘制出来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">plt.plot(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], <span class="string">'o'</span>, alpha=<span class="number">0.5</span>)</div><div class="line"><span class="keyword">for</span> length, vector <span class="keyword">in</span> zip(pca.explained_variance_, pca.components_):</div><div class="line">    v = vector * <span class="number">3</span> * np.sqrt(length)</div><div class="line">    plt.plot([<span class="number">0</span>, v[<span class="number">0</span>]], [<span class="number">0</span>, v[<span class="number">1</span>]], <span class="string">'-k'</span>, lw=<span class="number">3</span>)</div><div class="line">plt.axis(<span class="string">'equal'</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-041412.jpg" alt="png"></p>
<p>注意到一个向量比另外一个向量要长。直觉告诉我们这个方向比另外一个方向更加重要。各个主成分的方差（explained<em>variance</em>）量化了这些方向的重要程度。</p>
<p>我们可以认为第二重要的成分可以<strong>完全忽略</strong>也不会损失什么信息。我们看一下如果有95%的差异性被被保留的情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">clf = PCA(<span class="number">0.95</span>) <span class="comment"># keep 95% of variance</span></div><div class="line">X_trans = clf.fit_transform(X)</div><div class="line">print(X.shape)</div><div class="line">print(X_trans.shape)</div></pre></td></tr></table></figure>
<pre><code>(200, 2)
(200, 1)
</code></pre><p>通过丢弃5%的差异性，数据被压缩了50%。让我们来看一下压缩后的数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">X_new = clf.inverse_transform(X_trans)</div><div class="line">plt.plot(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], <span class="string">'o'</span>, alpha=<span class="number">0.2</span>)</div><div class="line">plt.plot(X_new[:, <span class="number">0</span>], X_new[:, <span class="number">1</span>], <span class="string">'ob'</span>, alpha=<span class="number">0.8</span>)</div><div class="line">plt.axis(<span class="string">'equal'</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-041429.jpg" alt="png"></p>
<p>浅色的点是原始数据，而深色的点是投影后的数据。我们可以看到通过丢弃数据集中5%的差异，然后重新投影，最重要的特征能够保留，并且数据也被压缩了50%！</p>
<h3 id="应用PCA到数字图形上"><a href="#应用PCA到数字图形上" class="headerlink" title="应用PCA到数字图形上"></a>应用PCA到数字图形上</h3><p>刚刚列举的基于二维数据的降维看上去有点抽象，但是在高维数据的可视化中投影和降维是非常有用的。让我们看下如何把PCA应用在我们之前见过的数字的例子上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line">digits = load_digits()</div><div class="line">X = digits.data</div><div class="line">y = digits.target</div><div class="line">print(X)</div></pre></td></tr></table></figure>
<pre><code>[[  0.   0.   5. ...,   0.   0.   0.]
 [  0.   0.   0. ...,  10.   0.   0.]
 [  0.   0.   0. ...,  16.   9.   0.]
 ..., 
 [  0.   0.   1. ...,   6.   0.   0.]
 [  0.   0.   2. ...,  12.   0.   0.]
 [  0.   0.  10. ...,  12.   1.   0.]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">pca = PCA(<span class="number">2</span>)  <span class="comment"># project from 64 to 2 dimensions</span></div><div class="line">Xproj = pca.fit_transform(X)</div><div class="line">print(X.shape)</div><div class="line">print(Xproj.shape)</div></pre></td></tr></table></figure>
<pre><code>(1797, 64)
(1797, 2)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">plt.scatter(Xproj[:, <span class="number">0</span>], Xproj[:, <span class="number">1</span>], c=y, edgecolor=<span class="string">'none'</span>, alpha=<span class="number">0.5</span>,</div><div class="line">            cmap=plt.cm.get_cmap(<span class="string">'nipy_spectral'</span>, <span class="number">10</span>))</div><div class="line">plt.colorbar();</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-041450.jpg" alt="png"></p>
<p>我们能从中发现数字之间的关系。本质上，我们把这些数字图形的64维空间摊平了。</p>
<h3 id="成分（Components）是什么意思？"><a href="#成分（Components）是什么意思？" class="headerlink" title="成分（Components）是什么意思？"></a>成分（Components）是什么意思？</h3><p>PCA是非常有用的降维算法，因为它是<em>特征向量</em>的直观表示。输入数据可以表示为一个向量，以下面的数字为例：</p>
<p>$$<br>x = [x_1, x_2, x_3 \cdots]<br>$$</p>
<p>它的真正含义是：</p>
<p>$$<br>image(x) = x_1 \cdot{\rm (pixel~1)} + x_2 \cdot{\rm (pixel~2)} + x_3 \cdot{\rm (pixel~3)} \cdots<br>$$</p>
<p>如果我们把像素空间的维度减少到6，我们只能复原部分的图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> fig_code.figures <span class="keyword">import</span> plot_image_components</div><div class="line"></div><div class="line">sns.set_style(<span class="string">'white'</span>)</div><div class="line">plot_image_components(digits.data[<span class="number">0</span>])</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-041550.jpg" alt="png"></p>
<p>像素级别的表示不是唯一的选择。我们也使用其他的<em>basis函数</em>，像下面的函数：</p>
<p>$$<br>image(x) = {\rm mean} + x_1 \cdot{\rm (basis~1)} + x_2 \cdot{\rm (basis~2)} + x_3 \cdot{\rm (basis~3)} \cdots<br>$$</p>
<p>PCA做的事情是对<strong>basis函数</strong>进行优化，以便用少的特征得到近似的估计。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> fig_code.figures <span class="keyword">import</span> plot_pca_interactive</div><div class="line">plot_pca_interactive(digits.data)</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-041610.jpg" alt="png"></p>
<p>这里我们看到我们用了6个成分对输入数字做了近似的估计！</p>
<p>因此我们从两方面来看待PCA。一方面把它看做<strong>维度约减</strong>，另一方面它能被作为一种<strong>数据压缩</strong>，去掉没用的噪音。通过这种方式，PCA也能被当做<strong>filtering</strong>。</p>
<h3 id="选择成分的数量"><a href="#选择成分的数量" class="headerlink" title="选择成分的数量"></a>选择成分的数量</h3><p>我们到底要丢弃多少信息呢？我们通过分析<strong>成分方差</strong>来找出答案：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sns.set()</div><div class="line">pca = PCA().fit(X)</div><div class="line">plt.plot(np.cumsum(pca.explained_variance_ratio_))</div><div class="line">plt.xlabel(<span class="string">'number of components'</span>)</div><div class="line">plt.ylabel(<span class="string">'cumulative explained variance'</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-041712.jpg" alt="png"></p>
<p>这里我们二维的投影损失了非常多的信息，我们需要20种成分来保留90%的差异性。</p>
<h3 id="用PCA做数据压缩"><a href="#用PCA做数据压缩" class="headerlink" title="用PCA做数据压缩"></a>用PCA做数据压缩</h3><p>我们刚才提到，PCA能作为数据压缩的一种方式。使用小的<code>n_components</code>允许你用主要的向量来表示高维的点。</p>
<p>这里通过改变成分的数量来观察数字0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">fig, axes = plt.subplots(<span class="number">8</span>, <span class="number">8</span>, figsize=(<span class="number">8</span>, <span class="number">8</span>))</div><div class="line">fig.subplots_adjust(hspace=<span class="number">0.1</span>, wspace=<span class="number">0.1</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> i, ax <span class="keyword">in</span> enumerate(axes.flat):</div><div class="line">    pca = PCA(i + <span class="number">1</span>).fit(X)</div><div class="line">    im = pca.inverse_transform(pca.transform(X[<span class="number">20</span>:<span class="number">21</span>]))</div><div class="line"></div><div class="line">    ax.imshow(im.reshape((<span class="number">8</span>, <span class="number">8</span>)), cmap=<span class="string">'binary'</span>)</div><div class="line">    ax.text(<span class="number">0.95</span>, <span class="number">0.05</span>, <span class="string">'n = &#123;0&#125;'</span>.format(i + <span class="number">1</span>), ha=<span class="string">'right'</span>,</div><div class="line">            transform=ax.transAxes, color=<span class="string">'green'</span>)</div><div class="line">    ax.set_xticks([])</div><div class="line">    ax.set_yticks([])</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-041728.jpg" alt="png"></p>
<p>让我们通过IPython的<code>interact</code>功能来看下当成分数量（n_components）变化时数字的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> IPython.html.widgets <span class="keyword">import</span> interact</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_digits</span><span class="params">(n_components)</span>:</span></div><div class="line">    fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</div><div class="line">    plt.subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, frameon=<span class="keyword">False</span>, xticks=[], yticks=[])</div><div class="line">    nside = <span class="number">10</span></div><div class="line">    </div><div class="line">    pca = PCA(n_components).fit(X)</div><div class="line">    Xproj = pca.inverse_transform(pca.transform(X[:nside ** <span class="number">2</span>]))</div><div class="line">    Xproj = np.reshape(Xproj, (nside, nside, <span class="number">8</span>, <span class="number">8</span>))</div><div class="line">    total_var = pca.explained_variance_ratio_.sum()</div><div class="line">    </div><div class="line">    im = np.vstack([np.hstack([Xproj[i, j] <span class="keyword">for</span> j <span class="keyword">in</span> range(nside)])</div><div class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> range(nside)])</div><div class="line">    plt.imshow(im)</div><div class="line">    plt.grid(<span class="keyword">False</span>)</div><div class="line">    plt.title(<span class="string">"n = &#123;0&#125;, variance = &#123;1:.2f&#125;"</span>.format(n_components, total_var),</div><div class="line">                 size=<span class="number">18</span>)</div><div class="line">    plt.clim(<span class="number">0</span>, <span class="number">16</span>)</div><div class="line">    </div><div class="line">interact(plot_digits, n_components=[<span class="number">1</span>, <span class="number">64</span>], nside=[<span class="number">1</span>, <span class="number">8</span>]);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-21-041753.jpg" alt="png"></p>
<hr>
<p><a href="https://github.com/yummybian/sklearn_pycon2015_zh.git" target="_blank" rel="external">Jupyter实现</a></p>
<hr>
<p><a rel="external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/3.0/88x31.png"></a><br>本作品采用<a rel="external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank">知识共享署名-非商业性使用-禁止演绎 3.0 未本地化版本许可协议</a>进行许可。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/12/03.2-Regression-Forests-zh/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="彬叔">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bin's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/12/03.2-Regression-Forests-zh/" itemprop="url">第四篇 深入监督学习：随机森林</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-12T21:54:10+08:00">
                2017-11-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/11/12/03.2-Regression-Forests-zh/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/11/12/03.2-Regression-Forests-zh/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/11/12/03.2-Regression-Forests-zh/" class="leancloud_visitors" data-flag-title="第四篇 深入监督学习：随机森林">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="深入监督学习：随机森林"><a href="#深入监督学习：随机森林" class="headerlink" title="深入监督学习：随机森林"></a>深入监督学习：随机森林</h1><p>前面我们见过了一种强大的分类器：<strong>支持向量机</strong>。<br>这里我们将看到另外一种强大的算法，这是一种<em>non-parametric</em>算法叫做<strong>随机森林</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</div><div class="line"></div><div class="line"><span class="comment"># use seaborn plotting defaults</span></div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns; sns.set()</div></pre></td></tr></table></figure>
<h2 id="随机森林：决策树"><a href="#随机森林：决策树" class="headerlink" title="随机森林：决策树"></a>随机森林：决策树</h2><p>随机森林基于决策树，它是<em>集成学习</em>的一个例子。基于这个原因，我们用决策树开始介绍。<br>决策树用一种非常直观的方式来分类对象：你可以问一系列问题来做出yes或no的判断。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> fig_code</div><div class="line">fig_code.plot_example_decision_tree()</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-042107.jpg" alt="png"></p>
<p>上面的二叉树分类很有效率。这个技巧的关键是<em>对特征提问</em>。<br>这个过程是这样的：在训练决策树分类器过程中，算法查找每一个特征，然后决定哪一个是正确的答案。</p>
<h3 id="创建决策树"><a href="#创建决策树" class="headerlink" title="创建决策树"></a>创建决策树</h3><p>这里是sklearn中的决策树分类器的一个例子。我们开始定义一些二维的打过标签的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</div><div class="line"></div><div class="line">X, y = make_blobs(n_samples=<span class="number">300</span>, centers=<span class="number">4</span>,</div><div class="line">                  random_state=<span class="number">0</span>, cluster_std=<span class="number">1.0</span>)</div><div class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'rainbow'</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-042121.jpg" alt="png"></p>
<p>我们引入一些可视化的函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> fig_code <span class="keyword">import</span> visualize_tree, plot_tree_interactive</div></pre></td></tr></table></figure>
<p>现在我们使用IPython的<code>interact</code>，来看一下决策树的分割过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">plot_tree_interactive(X, y);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-042139.jpg" alt="png"></p>
<p>注意随着深度的增加，这些节点一分为二，这些节点只包含一个分类。它是非常快速的<strong>非参数</strong>分类，在实践中很有用。</p>
<p><strong>问题：你能发现上面的有什么问题？ </strong></p>
<h3 id="决策树和过拟合"><a href="#决策树和过拟合" class="headerlink" title="决策树和过拟合"></a>决策树和过拟合</h3><p>决策树有个问题，就是它很容易产生过拟合。因为他们在分类的过程很容易学习到<strong>noise</strong>而不是<strong>signal</strong>。我们看下两棵决策树根据同一个数据集中两个不同子类构造的过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</div><div class="line">clf = DecisionTreeClassifier()</div><div class="line"></div><div class="line">plt.figure()</div><div class="line">visualize_tree(clf, X[:<span class="number">200</span>], y[:<span class="number">200</span>], boundaries=<span class="keyword">False</span>)</div><div class="line">plt.figure()</div><div class="line">visualize_tree(clf, X[<span class="number">-200</span>:], y[<span class="number">-200</span>:], boundaries=<span class="keyword">False</span>)</div></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.figure.Figure at 0x118962a50&gt;
</code></pre><p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-042200.jpg" alt="png"></p>
<pre><code>&lt;matplotlib.figure.Figure at 0x118f4de50&gt;
</code></pre><p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-042212.jpg" alt="png"></p>
<pre><code>&lt;matplotlib.figure.Figure at 0x11858d790&gt;
</code></pre><p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-042229.jpg" alt="png"></p>
<p>分类完全不同！很明显这产生了<strong>过拟合</strong>：当预测新的样本时，结果反映了模型中的<strong>noise</strong>而不是<strong>signal</strong>。</p>
<h2 id="Estimators的集成：随机森林"><a href="#Estimators的集成：随机森林" class="headerlink" title="Estimators的集成：随机森林"></a>Estimators的集成：随机森林</h2><p>一个解决过拟合的可以方式是使用<strong>集成方法</strong>：这是一个<em>meta-estimator</em>，它由多个独立的estimator组成，能够平衡每个estimator过拟合结果。和它包含的单独的estimator相比，评估的结果看上去更加健壮和精确。</p>
<p>一个最常见的集成方法叫做<strong>随机森林</strong>，它由许多决策树组成。</p>
<p>让我先看看下面的决策树例子获得一个感性的认识：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_randomized_tree</span><span class="params">(random_state=<span class="number">0</span>)</span>:</span></div><div class="line">    X, y = make_blobs(n_samples=<span class="number">300</span>, centers=<span class="number">4</span>,</div><div class="line">                      random_state=<span class="number">0</span>, cluster_std=<span class="number">2.0</span>)</div><div class="line">    clf = DecisionTreeClassifier(max_depth=<span class="number">15</span>)</div><div class="line">    </div><div class="line">    rng = np.random.RandomState(random_state)</div><div class="line">    i = np.arange(len(y))</div><div class="line">    rng.shuffle(i)</div><div class="line">    visualize_tree(clf, X[i[:<span class="number">250</span>]], y[i[:<span class="number">250</span>]], boundaries=<span class="keyword">False</span>,</div><div class="line">                   xlim=(X[:, <span class="number">0</span>].min(), X[:, <span class="number">0</span>].max()),</div><div class="line">                   ylim=(X[:, <span class="number">1</span>].min(), X[:, <span class="number">1</span>].max()))</div><div class="line">    </div><div class="line"><span class="keyword">from</span> IPython.html.widgets <span class="keyword">import</span> interact</div><div class="line">interact(fit_randomized_tree, random_state=[<span class="number">0</span>, <span class="number">100</span>]);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-042244.jpg" alt="png"></p>
<p>虽然样本发生变化时模型对样本的分类也变化了，但是大的特性保持不变。随机森林分类器做的事情类似，它综合考虑了所有决策树的判断：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</div><div class="line">clf = RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">0</span>)</div><div class="line">visualize_tree(clf, X, y, boundaries=<span class="keyword">False</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-042259.jpg" alt="png"></p>
<p>通过平衡100个随机模型，随机森林最后得出的模型更好拟合了数据。</p>
<h2 id="例子：移步到回归"><a href="#例子：移步到回归" class="headerlink" title="例子：移步到回归"></a>例子：移步到回归</h2><p>上面我们在分类问题上使用了随机森林。它在回归问题上也能很好地工作。使用的estimator叫做<code>sklearn.ensemble.RandomForestRegressor</code>。</p>
<p>让我们来看下怎么使用它：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</div><div class="line"></div><div class="line">x = <span class="number">10</span> * np.random.rand(<span class="number">100</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(x, sigma=<span class="number">0.3</span>)</span>:</span></div><div class="line">    fast_oscillation = np.sin(<span class="number">5</span> * x)</div><div class="line">    slow_oscillation = np.sin(<span class="number">0.5</span> * x)</div><div class="line">    noise = sigma * np.random.randn(len(x))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> slow_oscillation + fast_oscillation + noise</div><div class="line"></div><div class="line">y = model(x)</div><div class="line">plt.errorbar(x, y, <span class="number">0.3</span>, fmt=<span class="string">'o'</span>);</div></pre></td></tr></table></figure>
<p><img src="file:///Users/yummy/Desktop/03.2-Regression-Forests-zh/output_24_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">xfit = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">1000</span>)</div><div class="line">yfit = RandomForestRegressor(<span class="number">100</span>).fit(x[:, <span class="keyword">None</span>], y).predict(xfit[:, <span class="keyword">None</span>])</div><div class="line">ytrue = model(xfit, <span class="number">0</span>)</div><div class="line"></div><div class="line">plt.errorbar(x, y, <span class="number">0.3</span>, fmt=<span class="string">'o'</span>)</div><div class="line">plt.plot(xfit, yfit, <span class="string">'-r'</span>);</div><div class="line">plt.plot(xfit, ytrue, <span class="string">'-k'</span>, alpha=<span class="number">0.5</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-042322.jpg" alt="png"></p>
<p>我们可以看到，我们甚至没有指定一个multi-period模型，随机森林就能够足够灵活地适应了multi-period数据。</p>
<h2 id="例子：应用随机森林分类数字"><a href="#例子：应用随机森林分类数字" class="headerlink" title="例子：应用随机森林分类数字"></a>例子：应用随机森林分类数字</h2><p>我们之前见过了<strong>手写数字</strong>数据集。我们这里也用它来测试随机森林的准确率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line">digits = load_digits()</div><div class="line">digits.keys()</div></pre></td></tr></table></figure>
<pre><code>[&apos;images&apos;, &apos;data&apos;, &apos;target_names&apos;, &apos;DESCR&apos;, &apos;target&apos;]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">X = digits.data</div><div class="line">y = digits.target</div><div class="line">print(X.shape)</div><div class="line">print(y.shape)</div></pre></td></tr></table></figure>
<pre><code>(1797, 64)
(1797,)
</code></pre><p>首先我们看一下这些手写数字：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># set up the figure</span></div><div class="line">fig = plt.figure(figsize=(<span class="number">6</span>, <span class="number">6</span>))  <span class="comment"># figure size in inches</span></div><div class="line">fig.subplots_adjust(left=<span class="number">0</span>, right=<span class="number">1</span>, bottom=<span class="number">0</span>, top=<span class="number">1</span>, hspace=<span class="number">0.05</span>, wspace=<span class="number">0.05</span>)</div><div class="line"></div><div class="line"><span class="comment"># plot the digits: each image is 8x8 pixels</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">64</span>):</div><div class="line">    ax = fig.add_subplot(<span class="number">8</span>, <span class="number">8</span>, i + <span class="number">1</span>, xticks=[], yticks=[])</div><div class="line">    ax.imshow(digits.images[i], cmap=plt.cm.binary)</div><div class="line">    </div><div class="line">    <span class="comment"># label the image with the target value</span></div><div class="line">    ax.text(<span class="number">0</span>, <span class="number">7</span>, str(digits.target[i]))</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-042336.jpg" alt="png"></p>
<p>我们能快速使用决策树来对这些数字进行分类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</div><div class="line"></div><div class="line">Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=<span class="number">0</span>)</div><div class="line">clf = DecisionTreeClassifier(max_depth=<span class="number">11</span>)</div><div class="line">clf.fit(Xtrain, ytrain)</div><div class="line">ypred = clf.predict(Xtest)</div></pre></td></tr></table></figure>
<p>我们检查一下分类器准确率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">metrics.accuracy_score(ypred, ytest)</div></pre></td></tr></table></figure>
<pre><code>0.84222222222222221
</code></pre><p>用混淆矩阵更直观：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">plt.imshow(metrics.confusion_matrix(ypred, ytest),</div><div class="line">           interpolation=<span class="string">'nearest'</span>, cmap=plt.cm.binary)</div><div class="line">plt.grid(<span class="keyword">False</span>)</div><div class="line">plt.colorbar()</div><div class="line">plt.xlabel(<span class="string">"predicted label"</span>)</div><div class="line">plt.ylabel(<span class="string">"true label"</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-042349.jpg" alt="png"></p>
<h3 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h3><ol>
<li>用<code>sklearn.ensemble.RandomForestClassifier</code>来实现上面的任务。调节`max_depth<code>，</code>max_features<code>和</code>n_estimators``参数<br>会对结果造成什么影响？</li>
<li>再用<code>sklearn.svm.SVC</code>分类器，调节<code>kernel</code>, <code>C</code>和<code>gamma</code>试试看？</li>
<li>对每一个模型用一些参数集，然后检查一下F1分数（<code>sklearn.metrics.f1_score</code>）。</li>
</ol>
<hr>
<p><a href="https://github.com/yummybian/sklearn_pycon2015_zh.git" target="_blank" rel="external">Jupyter实现</a></p>
<hr>
<p><a rel="external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/3.0/88x31.png"></a><br>本作品采用<a rel="external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank">知识共享署名-非商业性使用-禁止演绎 3.0 未本地化版本许可协议</a>进行许可。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/12/03.1-Classification-SVMs-zh/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="彬叔">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bin's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/12/03.1-Classification-SVMs-zh/" itemprop="url">第三篇 深入监督学习：支持向量机</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-12T21:49:10+08:00">
                2017-11-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/11/12/03.1-Classification-SVMs-zh/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/11/12/03.1-Classification-SVMs-zh/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/11/12/03.1-Classification-SVMs-zh/" class="leancloud_visitors" data-flag-title="第三篇 深入监督学习：支持向量机">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="深入监督学习：支持向量机"><a href="#深入监督学习：支持向量机" class="headerlink" title="深入监督学习：支持向量机"></a>深入监督学习：支持向量机</h1><p>之前我们已经介绍了监督学习。有许多监督学习算法，这里我们介绍一种最强大且有意思的一种方法：<strong>支持向量机（SVMs）</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</div><div class="line"></div><div class="line"><span class="comment"># use seaborn plotting defaults</span></div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns; sns.set()</div><div class="line"></div><div class="line"><span class="comment"># eliminate warnings</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">warn</span><span class="params">(*args, **kwargs)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"><span class="keyword">import</span> warnings</div><div class="line">warnings.warn = warn</div></pre></td></tr></table></figure>
<h2 id="支持向量机的动机"><a href="#支持向量机的动机" class="headerlink" title="支持向量机的动机"></a>支持向量机的动机</h2><p>支持向量机（SVMs）是一种强大的监督学习算法，它可以用来进行分类和回归。SVMs是一种<strong>判别式</strong>分类器：用它可以在数据集之间划分边界。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</div><div class="line">X, y = make_blobs(n_samples=<span class="number">50</span>, centers=<span class="number">2</span>,</div><div class="line">                  random_state=<span class="number">0</span>, cluster_std=<span class="number">0.60</span>)</div><div class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'spring'</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-034506.jpg" alt="png"></p>
<p>一个判别式分类器试图在两个不同的数据集之间画一条直线。但我们马上发现，这类直线是有问题的，我们可以画出好几条直线来完美区分这两类数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">xfit = np.linspace(<span class="number">-1</span>, <span class="number">3.5</span>)</div><div class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'spring'</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> m, b <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">0.65</span>), (<span class="number">0.5</span>, <span class="number">1.6</span>), (<span class="number">-0.2</span>, <span class="number">2.9</span>)]:</div><div class="line">    plt.plot(xfit, m * xfit + b, <span class="string">'-k'</span>)</div><div class="line"></div><div class="line">plt.xlim(<span class="number">-1</span>, <span class="number">3.5</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-034531.jpg" alt="png"></p>
<p>在这些样本中，我们有三种分割方式。到底采用哪种分割方式，才能适应新的数据，这个真的很纠结！</p>
<p>我们如何才能改进它呢？</p>
<h3 id="支持向量机：最大化Margin"><a href="#支持向量机：最大化Margin" class="headerlink" title="支持向量机：最大化Margin"></a>支持向量机：最大化<em>Margin</em></h3><p>支持向量机可以解决这个问题。<br>支持向量机不仅仅画一条直线，还考虑了和这些直线相关的<em>region</em>，下面我们来看个例子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">xfit = np.linspace(<span class="number">-1</span>, <span class="number">3.5</span>)</div><div class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'spring'</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> m, b, d <span class="keyword">in</span> [(<span class="number">1</span>, <span class="number">0.65</span>, <span class="number">0.33</span>), (<span class="number">0.5</span>, <span class="number">1.6</span>, <span class="number">0.55</span>), (<span class="number">-0.2</span>, <span class="number">2.9</span>, <span class="number">0.2</span>)]:</div><div class="line">    yfit = m * xfit + b</div><div class="line">    plt.plot(xfit, yfit, <span class="string">'-k'</span>)</div><div class="line">    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor=<span class="string">'none'</span>, color=<span class="string">'#AAAAAA'</span>, alpha=<span class="number">0.4</span>)</div><div class="line"></div><div class="line">plt.xlim(<span class="number">-1</span>, <span class="number">3.5</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-034546.jpg" alt="png"></p>
<p>注意如果我们想最大化这个<em>region</em>，中间的直线貌似是最好的选择。<br>这是<strong>支持向量机</strong>直观表现，它通过数据集之间的<strong>margin</strong>来优化线性判别模型。</p>
<h4 id="拟合一个支持向量机"><a href="#拟合一个支持向量机" class="headerlink" title="拟合一个支持向量机"></a>拟合一个支持向量机</h4><p>现在我们将对这些点拟合一个支持向量机分类器。尽管SVM模型的数学细节很有意思，但是这里不会涉及。我们把sklearn中SVM算法当做一个黑盒来完成上面的任务。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC <span class="comment"># "Support Vector Classifier"</span></div><div class="line">clf = SVC(kernel=<span class="string">'linear'</span>)</div><div class="line">clf.fit(X, y)</div></pre></td></tr></table></figure>
<pre><code>SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma=&apos;auto&apos;, kernel=&apos;linear&apos;,
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
</code></pre><p>为了更好的可视化，我们写了一个函数来来绘制SVM决策边界：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_svc_decision_function</span><span class="params">(clf, ax=None)</span>:</span></div><div class="line">    <span class="string">"""Plot the decision function for a 2D SVC"""</span></div><div class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        ax = plt.gca()</div><div class="line">    x = np.linspace(plt.xlim()[<span class="number">0</span>], plt.xlim()[<span class="number">1</span>], <span class="number">30</span>)</div><div class="line">    y = np.linspace(plt.ylim()[<span class="number">0</span>], plt.ylim()[<span class="number">1</span>], <span class="number">30</span>)</div><div class="line">    Y, X = np.meshgrid(y, x)</div><div class="line">    P = np.zeros_like(X)</div><div class="line">    <span class="keyword">for</span> i, xi <span class="keyword">in</span> enumerate(x):</div><div class="line">        <span class="keyword">for</span> j, yj <span class="keyword">in</span> enumerate(y):</div><div class="line">            P[i, j] = clf.decision_function([xi, yj])</div><div class="line">    <span class="comment"># plot the margins</span></div><div class="line">    ax.contour(X, Y, P, colors=<span class="string">'k'</span>,</div><div class="line">               levels=[<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>], alpha=<span class="number">0.5</span>,</div><div class="line">               linestyles=[<span class="string">'--'</span>, <span class="string">'-'</span>, <span class="string">'--'</span>])</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">plt.scatter(X[:, <span class="number">0</span>].reshape(<span class="number">-1</span>, <span class="number">1</span>), X[:, <span class="number">1</span>].reshape(<span class="number">-1</span>, <span class="number">1</span>), c=y, s=<span class="number">50</span>, cmap=<span class="string">'spring'</span>)</div><div class="line">plot_svc_decision_function(clf);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-034615.jpg" alt="png"></p>
<p>注意虚线上面有一对点落在上面，这些点是这次拟合的关键，被称为<strong>支持向量</strong>。在sklearn中，它们被存储在分类器的<code>support_vectors_</code>属性中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'spring'</span>)</div><div class="line">plot_svc_decision_function(clf)</div><div class="line">plt.scatter(clf.support_vectors_[:, <span class="number">0</span>], clf.support_vectors_[:, <span class="number">1</span>],</div><div class="line">            s=<span class="number">200</span>, facecolors=<span class="string">'none'</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-034638.jpg" alt="png"></p>
<p>让我们用IPython的<code>interact</code>功能探索一下，点的分布是如何影响支持向量和判别拟合的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> IPython.html.widgets <span class="keyword">import</span> interact</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_svm</span><span class="params">(N=<span class="number">10</span>)</span>:</span></div><div class="line">    X, y = make_blobs(n_samples=<span class="number">200</span>, centers=<span class="number">2</span>,</div><div class="line">                      random_state=<span class="number">0</span>, cluster_std=<span class="number">0.60</span>)</div><div class="line">    X = X[:N]</div><div class="line">    y = y[:N]</div><div class="line">    clf = SVC(kernel=<span class="string">'linear'</span>)</div><div class="line">    clf.fit(X, y)</div><div class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'spring'</span>)</div><div class="line">    plt.xlim(<span class="number">-1</span>, <span class="number">4</span>)</div><div class="line">    plt.ylim(<span class="number">-1</span>, <span class="number">6</span>)</div><div class="line">    plot_svc_decision_function(clf, plt.gca())</div><div class="line">    plt.scatter(clf.support_vectors_[:, <span class="number">0</span>], clf.support_vectors_[:, <span class="number">1</span>],</div><div class="line">                s=<span class="number">200</span>, facecolors=<span class="string">'none'</span>)</div><div class="line">    </div><div class="line">interact(plot_svm, N=[<span class="number">10</span>, <span class="number">200</span>], kernel=<span class="string">'linear'</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-034652.jpg" alt="png"></p>
<p>注意如果你移动其他的任何点，如果它们没有越过决策边界，就不会对分类结果有影响。</p>
<h4 id="进一步看下Kernel方法"><a href="#进一步看下Kernel方法" class="headerlink" title="进一步看下Kernel方法"></a>进一步看下Kernel方法</h4><p>SVM还有一项很有用的特性叫做<em>kernels</em>。为了利用这个特性，我们看一些不能线性分割的数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_circles</div><div class="line">X, y = make_circles(<span class="number">100</span>, factor=<span class="number">.1</span>, noise=<span class="number">.1</span>)</div><div class="line"></div><div class="line">clf = SVC(kernel=<span class="string">'linear'</span>).fit(X, y)</div><div class="line"></div><div class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'spring'</span>)</div><div class="line">plot_svc_decision_function(clf);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-034712.jpg" alt="png"></p>
<p>很明显，只有非线性的才能划分这些数据。一种方式是我们采用<strong>kernel</strong>，它能对输入数据做些变换。例如，我们能用的一个简单模型叫做<strong>径向基函数核</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">r = np.exp(-(X[:, <span class="number">0</span>] ** <span class="number">2</span> + X[:, <span class="number">1</span>] ** <span class="number">2</span>))</div></pre></td></tr></table></figure>
<p>如果我们绘制出我们的数据，我们能看到它的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> mpl_toolkits <span class="keyword">import</span> mplot3d</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_3D</span><span class="params">(elev=<span class="number">30</span>, azim=<span class="number">30</span>)</span>:</span></div><div class="line">    ax = plt.subplot(projection=<span class="string">'3d'</span>)</div><div class="line">    ax.scatter3D(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], r, c=y, s=<span class="number">50</span>, cmap=<span class="string">'spring'</span>)</div><div class="line">    ax.view_init(elev=elev, azim=azim)</div><div class="line">    ax.set_xlabel(<span class="string">'x'</span>)</div><div class="line">    ax.set_ylabel(<span class="string">'y'</span>)</div><div class="line">    ax.set_zlabel(<span class="string">'r'</span>)</div><div class="line"></div><div class="line">interact(plot_3D, elev=[<span class="number">-90</span>, <span class="number">90</span>], azip=(<span class="number">-180</span>, <span class="number">180</span>));</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-034724.jpg" alt="png"></p>
<p>我们能看到加上了额外的维度，数据变得在很小范围可以线性可分。这是相对简单的核，SVM内置有一个更复杂的核。我们能使用它通过设置<code>kernel=&#39;rbf&#39;</code>，全称<em>径向基函数核</em>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">clf = SVC(kernel=<span class="string">'rbf'</span>)</div><div class="line">clf.fit(X, y)</div><div class="line"></div><div class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">50</span>, cmap=<span class="string">'spring'</span>)</div><div class="line">plot_svc_decision_function(clf)</div><div class="line">plt.scatter(clf.support_vectors_[:, <span class="number">0</span>], clf.support_vectors_[:, <span class="number">1</span>],</div><div class="line">            s=<span class="number">200</span>, facecolors=<span class="string">'none'</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-11-034736.jpg" alt="png"></p>
<hr>
<p><a href="https://github.com/yummybian/sklearn_pycon2015_zh.git" target="_blank" rel="external">Jupyter实现</a></p>
<hr>
<p><a rel="external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/3.0/88x31.png"></a><br>本作品采用<a rel="external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank">知识共享署名-非商业性使用-禁止演绎 3.0 未本地化版本许可协议</a>进行许可。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/09/02.2-Basic-Principles-zh/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="彬叔">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bin's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/09/02.2-Basic-Principles-zh/" itemprop="url">第二篇 机器学习的基本方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-09T22:34:10+08:00">
                2017-11-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/11/09/02.2-Basic-Principles-zh/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/11/09/02.2-Basic-Principles-zh/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/11/09/02.2-Basic-Principles-zh/" class="leancloud_visitors" data-flag-title="第二篇 机器学习的基本方法">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这里我们将深入机器学习的方法，和怎么通过sklearn API去使用它们。</p>
<p>在简单的介绍sklearn的<em>Estimator</em>对象后，我们会介绍<strong>监督学习</strong>，包括<strong>分类</strong>和<strong>回归</strong>问题，接下来<strong>无监督学习</strong>，包括<strong>降维</strong>和<strong>聚类</strong>问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># use seaborn for plot defaults</span></div><div class="line"><span class="comment"># this can be safely commented out</span></div><div class="line"><span class="keyword">import</span> seaborn; seaborn.set()</div><div class="line"></div><div class="line"><span class="comment"># eliminate warnings</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">warn</span><span class="params">(*args, **kwargs)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"><span class="keyword">import</span> warnings</div><div class="line">warnings.warn = warn</div></pre></td></tr></table></figure>
<h2 id="Sklearn-estimator对象"><a href="#Sklearn-estimator对象" class="headerlink" title="Sklearn  estimator对象"></a>Sklearn  estimator对象</h2><p>Sklearn中的每一个算法都被某一个’’Estimator’’对象所表示。例如一个线性回归被表示为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</div></pre></td></tr></table></figure>
<p><strong>Estimator参数</strong>：所有的estimator参数可以在初始化时被赋值，否则会用本身的默认值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">model = LinearRegression(normalize=<span class="keyword">True</span>)</div><div class="line">print(model.normalize)</div></pre></td></tr></table></figure>
<pre><code>True
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(model)</div></pre></td></tr></table></figure>
<pre><code>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)
</code></pre><p><strong>估计模型的参数</strong>：当数据已经拟合了estimator，模型参数就能很方便的估计出来。所有的参数都是estimator对象的属性，它们以下划线结尾。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = np.arange(<span class="number">10</span>)</div><div class="line">y = <span class="number">2</span> * x + <span class="number">1</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(x)</div><div class="line">print(y)</div></pre></td></tr></table></figure>
<pre><code>[0 1 2 3 4 5 6 7 8 9]
[ 1  3  5  7  9 11 13 15 17 19]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">plt.plot(x, y, <span class="string">'o'</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-09-142927.jpg" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># The input data for sklearn is 2D: (samples == 10 x features == 1)</span></div><div class="line">X = x[:, np.newaxis]</div><div class="line">print(X)</div><div class="line">print(y)</div></pre></td></tr></table></figure>
<pre><code>[[0]
 [1]
 [2]
 [3]
 [4]
 [5]
 [6]
 [7]
 [8]
 [9]]
[ 1  3  5  7  9 11 13 15 17 19]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># fit the model on our data</span></div><div class="line">model.fit(X, y)</div></pre></td></tr></table></figure>
<pre><code>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># underscore at the end indicates a fit parameter</span></div><div class="line">print(model.coef_)</div><div class="line">print(model.intercept_)</div></pre></td></tr></table></figure>
<pre><code>[ 2.]
1.0
</code></pre><p>我们可以看到一条我们期望的斜率为2，截距为1的直线。</p>
<h2 id="监督学习：分类和回归"><a href="#监督学习：分类和回归" class="headerlink" title="监督学习：分类和回归"></a>监督学习：分类和回归</h2><p>在<strong>监督学习</strong>中，我们拿到包含特征和标签的数据集。接下来的任务就是构造一个estimator能够根据特征来预测样本的标签。之前我们举了个简单的鸢尾花分类的例子。下面我们来看一些复杂的例子：</p>
<ul>
<li>给定一张通过天文望远镜拍摄的多色图，判断一个物体是恒星，还是类星体，或者是星系。</li>
<li>给定一张人物的照片，来识别照片中的人是谁。</li>
<li>给定人们看过的电影列表和相关评分，给他们推荐他们可能想看的电影</li>
</ul>
<p>上面的这些例子有一个或者多个的未知量，需要从已知量中推测出来。</p>
<p>监督学习进一步分为<strong>分类</strong>和<strong>回归</strong>这两类。<br>在<strong>分类</strong>中，标签是离散的；而在<strong>回归</strong>中标签是连续的。例如，在天文学中，判断一个物体是否是恒星，还是类星体，或者是星系就是分类问题，因为这三类标签是离散的。另一方面，我们想通过一些观测量来判断一个物体的年龄，它就是个回归问题，因为年龄标签是连续的。</p>
<h3 id="分类例子"><a href="#分类例子" class="headerlink" title="分类例子"></a>分类例子</h3><p>K最近邻接（kNN）是一种最简单的学习策略：给定一个新的，没见过的观测样本，在特征空间中找到的k个最相邻的样本，这些样本中的大多数属于某一个类别，则新样本也属于这个类别。</p>
<p>让我们用iris（鸢尾花）的例子做个测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors, datasets</div><div class="line"></div><div class="line">iris = datasets.load_iris()</div><div class="line">X, y = iris.data, iris.target</div><div class="line"></div><div class="line"><span class="comment"># create the model</span></div><div class="line">knn = neighbors.KNeighborsClassifier(n_neighbors=<span class="number">5</span>)</div><div class="line"></div><div class="line"><span class="comment"># fit the model</span></div><div class="line">knn.fit(X, y)</div><div class="line"></div><div class="line"><span class="comment"># What kind of iris has 3cm x 5cm sepal and 4cm x 2cm petal?</span></div><div class="line"><span class="comment"># call the "predict" method:</span></div><div class="line">result = knn.predict([[<span class="number">3</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">2</span>],])</div><div class="line">print(iris.target_names[result])</div></pre></td></tr></table></figure>
<pre><code>[&apos;versicolor&apos;]
</code></pre><p>You can also do probabilistic predictions:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">knn.predict_proba([[<span class="number">3</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">2</span>],])</div></pre></td></tr></table></figure>
<pre><code>array([[ 0. ,  0.8,  0.2]])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> fig_code <span class="keyword">import</span> plot_iris_knn</div><div class="line">plot_iris_knn()</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-09-142952.jpg" alt="png"></p>
<hr>
<h4 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h4><p>使用<code>sklearn.svm.SVC</code>解决上面的问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</div><div class="line"></div><div class="line"><span class="comment"># load the data</span></div><div class="line">iris = datasets.load_iris()</div><div class="line">X, y = iris.data, iris.target</div><div class="line"></div><div class="line">C = <span class="number">1.0</span>  <span class="comment"># SVM regularization parameter</span></div><div class="line">svc = SVC(kernel=<span class="string">'linear'</span>, C=C).fit(X, y)</div><div class="line"></div><div class="line"><span class="comment"># What kind of iris has 3cm x 5cm sepal and 4cm x 2cm petal?</span></div><div class="line"><span class="comment"># call the "predict" method:</span></div><div class="line">result = svc.predict([[<span class="number">3</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">2</span>],])</div><div class="line"></div><div class="line">print(iris.target_names[result])</div></pre></td></tr></table></figure>
<pre><code>[&apos;versicolor&apos;]
</code></pre><hr>
<h3 id="回归例子"><a href="#回归例子" class="headerlink" title="回归例子"></a>回归例子</h3><p>一个最简单的回归例子就是用直线来拟合数据，就像我们之前看到的那样。<br>sklearn也提供了很多很有用的回归算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create some simple data</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">np.random.seed(<span class="number">0</span>)</div><div class="line">X = np.random.random(size=(<span class="number">20</span>, <span class="number">1</span>))</div><div class="line">y = <span class="number">3</span> * X.squeeze() + <span class="number">2</span> + np.random.randn(<span class="number">20</span>)</div><div class="line"></div><div class="line">plt.plot(X.squeeze(), y, <span class="string">'o'</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-09-143010.jpg" alt="png"></p>
<p>上面的图我们可以用一条直线来拟合：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">model = LinearRegression()</div><div class="line">model.fit(X, y)</div><div class="line"></div><div class="line"><span class="comment"># Plot the data and the model prediction</span></div><div class="line">X_fit = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>)[:, np.newaxis]</div><div class="line">y_fit = model.predict(X_fit)</div><div class="line"></div><div class="line">plt.plot(X.squeeze(), y, <span class="string">'o'</span>)</div><div class="line">plt.plot(X_fit.squeeze(), y_fit);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-09-143047.jpg" alt="png"></p>
<p>sklearn也有很多的模型，它们能更好的表示数据集中的特征：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Fit a Random Forest</span></div><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</div><div class="line">model = RandomForestRegressor()</div><div class="line">model.fit(X, y)</div><div class="line"></div><div class="line"><span class="comment"># Plot the data and the model prediction</span></div><div class="line">X_fit = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>)[:, np.newaxis]</div><div class="line">y_fit = model.predict(X_fit)</div><div class="line"></div><div class="line">plt.plot(X.squeeze(), y, <span class="string">'o'</span>)</div><div class="line">plt.plot(X_fit.squeeze(), y_fit);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-09-143107.jpg" alt="png"></p>
<p>上面这个模型貌似“更好”的拟合了数据，后面我们将讨论如何选择一个模型的细节。</p>
<hr>
<h4 id="练习-1"><a href="#练习-1" class="headerlink" title="练习"></a>练习</h4><p>探索<code>RandomForestRegressor</code>对象，它里面有哪些参数？如果改变了这些参数上面的图会发生怎样的变化？</p>
<hr>
<h2 id="无监督学习：降维和聚类"><a href="#无监督学习：降维和聚类" class="headerlink" title="无监督学习：降维和聚类"></a>无监督学习：降维和聚类</h2><p><strong>无监督学习</strong>致力于解决不同分类的问题。这类数据没有标签，我们对他们之间的相似性很感兴趣。在某种意义上，你能认为无监督学习是数据自身发现标签的过程。无监督学习有<strong>降维</strong>，<strong>聚类</strong>和<strong>密度估计</strong>这三部分组成。下面是一些无监督学习的问题：</p>
<ul>
<li>给定遥远星系的详细观察数据，决定哪一种特征或者特征混合集能最好地对这类数据进行总结。</li>
<li>给定两种声音源的混合（例如，一种人声和背景音乐），然后把它们分离开来。</li>
<li>给定一段视频，提取出移动的物体，把它和之前见过的移动物体进行分类。</li>
</ul>
<p>有时无监督和监督这两种方法可以结合起来：例如，无监督用来在各式各样的数据中发现有用的特征，然后把这些特征应用到一种监督学习框架中。</p>
<h3 id="降维：PCA"><a href="#降维：PCA" class="headerlink" title="降维：PCA"></a>降维：PCA</h3><p>主成分分析（PCA）是一种降维技术，它能发现特征之间的差异然后对特征进行混合而达到降维的目的。</p>
<p>看下iris数据集。因为有四种特征，所以不能用2D plot来可视化。但可以通过对花瓣和花萼的宽高特征进行混合来展示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">X, y = iris.data, iris.target</div><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line">pca = PCA(n_components=<span class="number">2</span>)</div><div class="line">pca.fit(X)</div><div class="line">X_reduced = pca.transform(X)</div><div class="line">print(<span class="string">"Reduced dataset shape:"</span>, X_reduced.shape)</div><div class="line"></div><div class="line">print(X_reduced.shape)</div><div class="line">print(X_reduced[<span class="number">0</span>])</div><div class="line"><span class="keyword">import</span> pylab <span class="keyword">as</span> pl</div><div class="line">pl.scatter(X_reduced[:, <span class="number">0</span>], X_reduced[:, <span class="number">1</span>], c=y,</div><div class="line">           cmap=<span class="string">'RdYlBu'</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> component <span class="keyword">in</span> pca.components_:  </div><div class="line">    print(<span class="string">" + "</span>.join(<span class="string">"%.3f x %s"</span> % (value, name)</div><div class="line">                     <span class="keyword">for</span> value, name <span class="keyword">in</span> zip(component,</div><div class="line">                                            iris.feature_names)))</div></pre></td></tr></table></figure>
<pre><code>(&apos;Reduced dataset shape:&apos;, (150, 2))
(150, 2)
[-2.68420713  0.32660731]
0.362 x sepal length (cm) + -0.082 x sepal width (cm) + 0.857 x petal length (cm) + 0.359 x petal width (cm)
0.657 x sepal length (cm) + 0.730 x sepal width (cm) + -0.176 x petal length (cm) + -0.075 x petal width (cm)
</code></pre><p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-09-143133.jpg" alt="png"></p>
<h4 id="聚类-K-means"><a href="#聚类-K-means" class="headerlink" title="聚类: K-means"></a>聚类: K-means</h4><p>聚类使用一个给定的标准把具有相同性质的样本进行分组</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</div><div class="line">k_means = KMeans(n_clusters=<span class="number">3</span>, random_state=<span class="number">0</span>) <span class="comment"># Fixing the RNG in kmeans</span></div><div class="line">k_means.fit(X)</div><div class="line">y_pred = k_means.predict(X)</div><div class="line"></div><div class="line">pl.scatter(X_reduced[:, <span class="number">0</span>], X_reduced[:, <span class="number">1</span>], c=y_pred,</div><div class="line">           cmap=<span class="string">'RdYlBu'</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-09-143153.jpg" alt="png"></p>
<h3 id="总结：sklearn-estimator接口"><a href="#总结：sklearn-estimator接口" class="headerlink" title="总结：sklearn estimator接口"></a>总结：sklearn estimator接口</h3><p>sklearn确保在所有的方法中使用统一的接口，下面我们看一些这样的例子。给定sklearn <strong>estimator</strong>对象叫做<code>model</code>，有下面的方法：</p>
<ul>
<li>在<strong>所有estimator</strong>中可用的方法：<ul>
<li><code>model.fit()</code>: 用来拟合训练数据。对于监督学习的应用，它接受两个参数：数据<code>X</code>和标签<code>y</code>（例如：<code>model.fit(X, y)</code>）。对于无监督应用，它只接受一个数据参数<code>X</code>（例如：<code>model.fit(X)</code>）。</li>
</ul>
</li>
<li>在<strong>监督estimators</strong>中可用的方法：<ul>
<li><code>model.predict()</code>：给定一个训练好的模型，预测新数据集的标签。这个方法接受一个新数据集参数<code>X_new</code>（例如：<code>model.predict(X_new)</code>），返回一个新数据集的标签数组。</li>
<li><code>model.predict_proba()</code>：返回每个标签对应的概率，而<code>model.predict()</code>返回那个最大概率的标签。</li>
<li><code>model.score()</code>： 对应分类和回归问题，大部分estimator实现了这个方法。它返回一个0到1之间的分数，分数越高表示拟合的越好。</li>
</ul>
</li>
<li>在<strong>无监督estimator</strong>中可用的方法：<ul>
<li><code>model.predict()</code>：在聚类算法中预测标签。</li>
<li><code>model.transform()</code>：在无监督模型中，对新数据集进行标准化。它接受一个<code>X_new</code>参数，返回标准化后的数据集。</li>
<li><code>model.fit_transform()</code>：先拟合，后标准化数据集。</li>
</ul>
</li>
</ul>
<h2 id="模型验证"><a href="#模型验证" class="headerlink" title="模型验证"></a>模型验证</h2><p>在机器学习中一个重要的步骤就是<strong>模型验证</strong>，它用来验证你训练好的模型的泛化能力。让我们看一下<strong>最近邻接分类器</strong>的例子。这是一个非常简单的分类器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</div><div class="line">X, y = iris.data, iris.target</div><div class="line">clf = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</div><div class="line">clf.fit(X, y)</div><div class="line">y_pred = clf.predict(X)</div><div class="line">print(np.all(y == y_pred))</div></pre></td></tr></table></figure>
<pre><code>True
</code></pre><p>一个更有用的方式就是通过<strong>混淆矩阵</strong>观测分类结果。下面的矩阵，看到只有对角线有数据，说明全部分类正确。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</div><div class="line">print(confusion_matrix(y, y_pred))</div></pre></td></tr></table></figure>
<pre><code>[[50  0  0]
 [ 0 50  0]
 [ 0  0 50]]
</code></pre><p>对于每一类，所有50个训练样本被正确地分类。但这并不意味着我们的模型是完美的。这个模型泛化能力比较弱。我们分割<strong>训练集</strong>和<strong>测试集</strong>来模拟一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</div><div class="line"></div><div class="line">Xtrain, Xtest, ytrain, ytest = train_test_split(X, y)</div><div class="line">clf.fit(Xtrain, ytrain)</div><div class="line">ypred = clf.predict(Xtest)</div><div class="line">print(confusion_matrix(ytest, ypred))</div></pre></td></tr></table></figure>
<pre><code>[[10  0  0]
 [ 0 16  1]
 [ 0  0 11]]
</code></pre><p>以上展示了我们分类器的真实性能，看上去第二类和第三类有些分不清。把数据分为训练集和测试集是非常重要的，后面我们会深入讨论模型评估。</p>
<h2 id="流程图：如何选择你的Estimator"><a href="#流程图：如何选择你的Estimator" class="headerlink" title="流程图：如何选择你的Estimator"></a>流程图：如何选择你的Estimator</h2><p>这是一张sklearn的顶级贡献者<a href="https://github.com/amueller" target="_blank" rel="external">Andreas Mueller</a>绘制的流程图，它告诉你在不同的场景下如何选择合适的算法。请大家保存起来随时参考。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</div><div class="line">Image(<span class="string">"http://scikit-learn.org/dev/_static/ml_map.png"</span>)</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-10-output_48_0.png" alt="png"></p>
<p>Original source on the <a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/" target="_blank" rel="external">scikit-learn website</a></p>
<h2 id="快速应用：OCR识别"><a href="#快速应用：OCR识别" class="headerlink" title="快速应用：OCR识别"></a>快速应用：OCR识别</h2><p>我们根据上面的原则展示一个更有趣的例子，OCR识别</p>
<ul>
<li>就是识别手写的数字<br>这个问题涉及在一副图片中查找和识别数字。这里为了方便，我们引用了sklearn库中预处理过的数字。</li>
</ul>
<h3 id="加载和可视化数字"><a href="#加载和可视化数字" class="headerlink" title="加载和可视化数字"></a>加载和可视化数字</h3><p>我们将使用sklearn接口大致预览一下这些数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line">digits = datasets.load_digits()</div><div class="line">digits.images.shape</div></pre></td></tr></table></figure>
<pre><code>(1797, 8, 8)
</code></pre><p>我们绘制出部分数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">fig, axes = plt.subplots(<span class="number">10</span>, <span class="number">10</span>, figsize=(<span class="number">8</span>, <span class="number">8</span>))</div><div class="line">fig.subplots_adjust(hspace=<span class="number">0.1</span>, wspace=<span class="number">0.1</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> i, ax <span class="keyword">in</span> enumerate(axes.flat):</div><div class="line">    ax.imshow(digits.images[i], cmap=<span class="string">'binary'</span>)</div><div class="line">    ax.text(<span class="number">0.05</span>, <span class="number">0.05</span>, str(digits.target[i]),</div><div class="line">            transform=ax.transAxes, color=<span class="string">'green'</span>)</div><div class="line">    ax.set_xticks([])</div><div class="line">    ax.set_yticks([])</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-09-143219.jpg" alt="png"></p>
<p>这里数字表示为8*8的像素值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># The images themselves</span></div><div class="line">print(digits.images.shape)</div><div class="line">print(digits.images[<span class="number">0</span>])</div></pre></td></tr></table></figure>
<pre><code>(1797, 8, 8)
[[  0.   0.   5.  13.   9.   1.   0.   0.]
 [  0.   0.  13.  15.  10.  15.   5.   0.]
 [  0.   3.  15.   2.   0.  11.   8.   0.]
 [  0.   4.  12.   0.   0.   8.   8.   0.]
 [  0.   5.   8.   0.   0.   9.   8.   0.]
 [  0.   4.  11.   0.   1.  12.   7.   0.]
 [  0.   2.  14.   5.  10.  12.   0.   0.]
 [  0.   0.   6.  13.  10.   0.   0.   0.]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># The data for use in our algorithms</span></div><div class="line">print(digits.data.shape)</div><div class="line">print(digits.data[<span class="number">0</span>])</div></pre></td></tr></table></figure>
<pre><code>(1797, 64)
[  0.   0.   5.  13.   9.   1.   0.   0.   0.   0.  13.  15.  10.  15.   5.
   0.   0.   3.  15.   2.   0.  11.   8.   0.   0.   4.  12.   0.   0.   8.
   8.   0.   0.   5.   8.   0.   0.   9.   8.   0.   0.   4.  11.   0.   1.
  12.   7.   0.   0.   2.  14.   5.  10.  12.   0.   0.   0.   0.   6.  13.
  10.   0.   0.   0.]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># The target label</span></div><div class="line">print(digits.target)</div></pre></td></tr></table></figure>
<pre><code>[0 1 2 ..., 8 9 8]
</code></pre><p>我们的数据含有1797个样本，每个样本有64维。</p>
<h3 id="无监督学习：降维"><a href="#无监督学习：降维" class="headerlink" title="无监督学习：降维"></a>无监督学习：降维</h3><p>我们想可视化64维的参数空间，但把它绘制出来很难。于是乎我们使用一种无监督的方法把它降成2维的。这里，我们将使用一种称为<em>Isomap</em>的manifold算法，来转换成2维的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> Isomap</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">iso = Isomap(n_components=<span class="number">2</span>)</div><div class="line">data_projected = iso.fit_transform(digits.data)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">data_projected.shape</div></pre></td></tr></table></figure>
<pre><code>(1797, 2)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">plt.scatter(data_projected[:, <span class="number">0</span>], data_projected[:, <span class="number">1</span>], c=digits.target,</div><div class="line">            edgecolor=<span class="string">'none'</span>, alpha=<span class="number">0.5</span>, cmap=plt.cm.get_cmap(<span class="string">'nipy_spectral'</span>, <span class="number">10</span>));</div><div class="line">plt.colorbar(label=<span class="string">'digit label'</span>, ticks=range(<span class="number">10</span>))</div><div class="line">plt.clim(<span class="number">-0.5</span>, <span class="number">9.5</span>)</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-09-143237.jpg" alt="png"></p>
<p>我们看到了数字在2维的参数空间中被很好的分割开来，这样我们就可以采用监督的分类算法。</p>
<h3 id="数字分类"><a href="#数字分类" class="headerlink" title="数字分类"></a>数字分类</h3><p>我们尝试对数字进行分类。首先我们把数字分为训练集和测试集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</div><div class="line">Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,</div><div class="line">                                                random_state=<span class="number">2</span>)</div><div class="line">print(Xtrain.shape, Xtest.shape)</div></pre></td></tr></table></figure>
<pre><code>((1347, 64), (450, 64))
</code></pre><p>让我们采用一个简单的logistic回归算法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</div><div class="line">clf = LogisticRegression(penalty=<span class="string">'l2'</span>)</div><div class="line">clf.fit(Xtrain, ytrain)</div><div class="line">ypred = clf.predict(Xtest)</div><div class="line">print(ytest)</div><div class="line">print(ypred)</div></pre></td></tr></table></figure>
<pre><code>[4 0 9 1 4 7 1 5 1 6 6 7 6 1 5 5 4 6 2 7 4 6 4 1 5 2 9 5 4 6 5 6 3 4 0 9 9
 8 4 6 8 8 5 7 9 6 9 6 1 3 0 1 9 7 3 3 1 1 8 8 9 8 5 4 4 7 3 5 8 4 3 1 3 8
 7 3 3 0 8 7 2 8 5 3 8 7 6 4 6 2 2 0 1 1 5 3 5 7 6 8 2 2 6 4 6 7 3 7 3 9 4
 7 0 3 5 8 5 0 3 9 2 7 3 2 0 8 1 9 2 1 9 1 0 3 4 3 0 9 3 2 2 7 3 1 6 7 2 8
 3 1 1 6 4 8 2 1 8 4 1 3 1 1 9 5 4 8 7 4 8 9 5 7 6 9 0 0 4 0 0 4 0 6 5 8 8
 3 7 9 2 0 3 2 7 3 0 2 1 5 2 7 0 6 9 3 1 1 3 5 2 3 5 2 1 2 9 4 6 5 5 5 9 7
 1 5 9 6 3 7 1 7 5 1 7 2 7 5 5 4 8 6 6 2 8 7 3 7 8 0 9 5 7 4 3 4 1 0 3 3 5
 4 1 3 1 2 5 1 4 0 3 1 5 5 7 4 0 1 0 8 5 5 5 4 0 1 8 6 2 1 1 1 7 9 6 7 9 7
 0 4 9 6 9 2 7 2 1 0 8 2 8 6 5 7 8 4 5 7 8 6 5 2 6 9 3 0 0 8 0 6 6 7 1 4 5
 6 9 7 2 8 5 1 2 4 1 8 8 7 6 0 8 0 6 5 5 7 8 0 4 1 4 5 9 2 2 3 9 1 3 9 3 2
 8 0 6 5 6 2 5 2 3 2 6 1 0 7 6 0 6 2 7 0 3 2 4 2 9 6 9 7 7 0 3 5 4 1 2 2 1
 2 7 7 0 4 9 8 5 6 1 6 5 2 0 8 2 4 3 3 2 9 3 8 9 9 3 9 0 3 4 7 9 1 5 7 5 0
 5 3 5 0 2 7]
[4 0 9 1 8 7 1 5 1 6 6 7 6 1 5 5 1 6 2 7 4 6 4 1 5 2 9 5 4 6 5 6 3 4 0 9 9
 8 4 6 8 1 5 7 9 8 9 6 1 7 0 1 9 7 3 3 1 8 8 8 9 8 5 1 4 8 7 5 8 4 3 9 3 8
 7 3 3 0 8 7 2 8 5 3 8 7 6 4 6 2 2 0 1 1 5 3 5 7 6 8 2 2 6 4 6 7 3 7 3 9 4
 7 0 3 5 1 5 0 3 9 2 7 7 2 0 8 1 9 2 1 5 1 0 3 4 3 0 8 3 2 2 7 3 1 6 7 2 8
 3 1 1 6 4 8 2 1 8 4 8 3 1 1 9 5 4 8 7 4 8 9 5 7 6 9 0 0 4 0 0 9 0 6 5 8 8
 3 7 8 2 0 8 2 7 3 0 2 1 5 2 7 0 6 9 3 3 1 3 5 2 5 5 2 1 2 9 4 6 5 5 5 9 7
 1 5 7 6 3 7 1 7 5 1 7 2 7 5 5 4 8 6 6 2 8 7 3 7 8 0 3 5 7 4 3 4 1 0 3 3 5
 4 1 3 1 2 5 1 4 0 3 1 5 5 7 4 0 1 0 8 5 5 5 4 0 1 8 6 2 1 1 1 7 9 6 7 9 7
 0 4 9 6 9 2 7 2 1 0 8 2 8 6 5 7 8 4 5 7 8 6 5 2 6 9 3 0 0 8 0 6 6 7 1 4 5
 6 9 7 2 8 5 1 2 4 1 8 8 7 6 0 8 0 6 5 5 7 8 0 4 1 4 5 9 2 2 3 9 1 3 9 3 2
 8 0 6 5 6 2 5 2 3 2 6 1 0 7 6 0 6 2 7 0 3 2 4 2 9 6 9 7 7 0 3 5 4 1 2 2 1
 2 7 7 0 4 9 8 5 6 1 6 5 2 0 8 2 4 3 3 2 9 3 8 9 9 5 9 0 3 4 7 9 8 5 7 5 0
 5 3 5 0 2 7]
</code></pre><p>通过和测试集的分类标签进行比较，我们看下分类的精确度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</div><div class="line">accuracy_score(ytest, ypred)</div></pre></td></tr></table></figure>
<pre><code>0.94666666666666666
</code></pre><p>这个数字没有告诉我们哪些类分错了，一个好的方法就是使用<em>混淆矩阵</em>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</div><div class="line">print(confusion_matrix(ytest, ypred))</div></pre></td></tr></table></figure>
<pre><code>[[42  0  0  0  0  0  0  0  0  0]
 [ 0 45  0  1  0  0  0  0  3  1]
 [ 0  0 47  0  0  0  0  0  0  0]
 [ 0  0  0 42  0  2  0  3  1  0]
 [ 0  2  0  0 36  0  0  0  1  1]
 [ 0  0  0  0  0 52  0  0  0  0]
 [ 0  0  0  0  0  0 42  0  1  0]
 [ 0  0  0  0  0  0  0 48  1  0]
 [ 0  2  0  0  0  0  0  0 38  0]
 [ 0  0  0  1  0  1  0  1  2 34]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">plt.imshow(np.log(confusion_matrix(ytest, ypred)),</div><div class="line">           cmap=<span class="string">'Blues'</span>, interpolation=<span class="string">'nearest'</span>)</div><div class="line">plt.grid(<span class="keyword">False</span>)</div><div class="line">plt.ylabel(<span class="string">'true'</span>)</div><div class="line">plt.xlabel(<span class="string">'predicted'</span>);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-10-output_75_0.png" alt="png"></p>
<p>我们把分好类的数字绘制出来，红色的数字表示错误的分类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">fig, axes = plt.subplots(<span class="number">10</span>, <span class="number">10</span>, figsize=(<span class="number">8</span>, <span class="number">8</span>))</div><div class="line">fig.subplots_adjust(hspace=<span class="number">0.1</span>, wspace=<span class="number">0.1</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> i, ax <span class="keyword">in</span> enumerate(axes.flat):</div><div class="line">    ax.imshow(Xtest[i].reshape(<span class="number">8</span>, <span class="number">8</span>), cmap=<span class="string">'binary'</span>)</div><div class="line">    ax.text(<span class="number">0.05</span>, <span class="number">0.05</span>, str(ypred[i]),</div><div class="line">            transform=ax.transAxes,</div><div class="line">            color=<span class="string">'green'</span> <span class="keyword">if</span> (ytest[i] == ypred[i]) <span class="keyword">else</span> <span class="string">'red'</span>)</div><div class="line">    ax.set_xticks([])</div><div class="line">    ax.set_yticks([])</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-09-143319.jpg" alt="png"></p>
<p>这个例子只用了简单的logistic回归算法就搞定了，有很多方式可以改进我们的分类器。进一步我们将使用更复杂的模型，使用交叉验证，和其他技术。这些在教程的后面会涉及。</p>
<hr>
<p><a href="https://github.com/yummybian/sklearn_pycon2015_zh.git" target="_blank" rel="external">Jupyter实现</a></p>
<hr>
<p><a rel="external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/3.0/88x31.png"></a><br>本作品采用<a rel="external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank">知识共享署名-非商业性使用-禁止演绎 3.0 未本地化版本许可协议</a>进行许可。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/09/02.1-Machine-Learning-Intro-zh/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="彬叔">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bin's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/09/02.1-Machine-Learning-Intro-zh/" itemprop="url">第一篇 介绍Scikit-Learn：用Python进行机器学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-09T21:36:15+08:00">
                2017-11-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/11/09/02.1-Machine-Learning-Intro-zh/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/11/09/02.1-Machine-Learning-Intro-zh/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/11/09/02.1-Machine-Learning-Intro-zh/" class="leancloud_visitors" data-flag-title="第一篇 介绍Scikit-Learn：用Python进行机器学习">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><p><strong>主要目标</strong>：介绍机器学习核心概念，和如何使用sklearn包。</p>
<ul>
<li>机器学习的定义</li>
<li>sklearn中的数据表示</li>
<li>sklearn API介绍</li>
</ul>
<h2 id="关于Scikit-Learn"><a href="#关于Scikit-Learn" class="headerlink" title="关于Scikit-Learn"></a>关于Scikit-Learn</h2><p><a href="http://scikit-learn.org" target="_blank" rel="external">sklearn</a>用Python实现了许多众所周知的机器学习算法，并且提供清晰和成熟的API接口。全世界达数百人参与贡献了sklearn的代码，它在工业和学术界被大量应用。<br>sklearn基于<a href="www.numpy.org">NumPy</a>和<a href="http://scipy.org" target="_blank" rel="external">SciPy</a>，这两个库在数组处理和科学计算方面很强大。此外sklearn不适合大数据集处理，尽管在这方面已经做了一些<a href="https://github.com/ogrisel/parallel_ml_tutorial" target="_blank" rel="external">工作</a>。</p>
<h1 id="什么是机器学习？"><a href="#什么是机器学习？" class="headerlink" title="什么是机器学习？"></a>什么是机器学习？</h1><p>在这部分我们将开始探索基本的机器学习规则。机器学习通过调整参数来学习已知的数据，从而建立模型来预测新的数据。机器学习作为人工智能的一个领域，通过某种程度的泛化使得计算机更加智能。</p>
<p>这里我们将看一下两个非常简单的例子。第一个是<strong>分类</strong>,图片显示一个二维数据的集合，不同的颜色表示不同的分类。一个分类算法可以划分两种不同颜色的点集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"></div><div class="line"><span class="comment"># set seaborn plot defaults.</span></div><div class="line"><span class="comment"># This can be safely commented out</span></div><div class="line"><span class="keyword">import</span> seaborn; seaborn.set()</div><div class="line"></div><div class="line"><span class="comment"># eliminate warnings</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">warn</span><span class="params">(*args, **kwargs)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"><span class="keyword">import</span> warnings</div><div class="line">warnings.warn = warn</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Import the example plot from the figures directory</span></div><div class="line"><span class="keyword">from</span> fig_code <span class="keyword">import</span> plot_sgd_separator</div><div class="line">plot_sgd_separator()</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-09-133734.jpg" alt="png"></p>
<p>这貌似像一个简单的任务，但大道至简。通过画出分割线形成模型，我们可以让这个模型泛化到新的数据集，它可以用来预测分割新的点集（红色和蓝色）。</p>
<p>如果你想要查看源码，可以使用<code>%load</code>命令。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># %load fig_code/sgd_separator.py</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</div><div class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_sgd_separator</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># we create 50 separable points</span></div><div class="line">    X, Y = make_blobs(n_samples=<span class="number">50</span>, centers=<span class="number">2</span>,</div><div class="line">                      random_state=<span class="number">0</span>, cluster_std=<span class="number">0.60</span>)</div><div class="line"></div><div class="line">    <span class="comment"># fit the model</span></div><div class="line">    clf = SGDClassifier(loss=<span class="string">"hinge"</span>, alpha=<span class="number">0.01</span>,</div><div class="line">                        n_iter=<span class="number">200</span>, fit_intercept=<span class="keyword">True</span>)</div><div class="line">    clf.fit(X, Y)</div><div class="line"></div><div class="line">    <span class="comment"># plot the line, the points, and the nearest vectors to the plane</span></div><div class="line">    xx = np.linspace(<span class="number">-1</span>, <span class="number">5</span>, <span class="number">10</span>)</div><div class="line">    yy = np.linspace(<span class="number">-1</span>, <span class="number">5</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line">    X1, X2 = np.meshgrid(xx, yy)</div><div class="line">    Z = np.empty(X1.shape)</div><div class="line">    <span class="keyword">for</span> (i, j), val <span class="keyword">in</span> np.ndenumerate(X1):</div><div class="line">        x1 = val</div><div class="line">        x2 = X2[i, j]</div><div class="line">        p = clf.decision_function(np.array([[x1, x2]]))</div><div class="line">        Z[i, j] = p[<span class="number">0</span>]</div><div class="line">    levels = [<span class="number">-1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>]</div><div class="line">    linestyles = [<span class="string">'dashed'</span>, <span class="string">'solid'</span>, <span class="string">'dashed'</span>]</div><div class="line">    colors = <span class="string">'k'</span></div><div class="line"></div><div class="line">    ax = plt.axes()</div><div class="line">    ax.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)</div><div class="line">    ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y, cmap=plt.cm.Paired)</div><div class="line"></div><div class="line">    ax.axis(<span class="string">'tight'</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    plot_sgd_separator()</div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
<p>下一个我们看下<strong>回归</strong>的例子，数据集的最佳拟合线</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> fig_code <span class="keyword">import</span> plot_linear_regression</div><div class="line">plot_linear_regression()</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-09-133832.jpg" alt="png"></p>
<p>我们又看到了一个拟合数据的模型，这个模型也可以泛化到新的数据。这个模型通过学习训练数据，从而预测测试集的结果：给它x值，它可以预测y值。</p>
<h2 id="数据在Scikit-learn中的表示"><a href="#数据在Scikit-learn中的表示" class="headerlink" title="数据在Scikit-learn中的表示"></a>数据在Scikit-learn中的表示</h2><p>机器学习通过数据建立模型，基于这点，我们开始讨论，数据如何表示才能让计算机可以理解。我们将举一些matplotlib的例子并且可视化它们。</p>
<p>在sklearn中的大多数机器学习算法的数据存储在二维的数组或者矩阵中。有的采用<code>numpy arrays</code>，还有的采用<code>scipy.sparse</code>矩阵。数组的大小设定为<code>[n_samples, n_features]</code></p>
<ul>
<li><strong>n_samples:</strong> 样本的个数，每个样本是待处理的一项数据。它可以是一篇文档，一张图片，一段音频，一段视频，数据库或CSV文件中的某一行，甚至一个你能描述其特征的可量化的集合。</li>
<li><strong>n_features</strong> 特征的数目，用一种可以量化的方式来描述每一个样本的特征数目。它一般是实数，在一些场合下可以为布尔值或者离散的数值。</li>
</ul>
<p>特征的数量必须是提前确定的。它可能有很高的维度（例如：上百万的维度），其中一些样本中大部分的特征值为零。在这种场景下scipy.sparse矩阵就很有用了，它比numpy arrays更加节约内存。</p>
<h2 id="一个简单的例子：Iris（鸢尾花）数据集"><a href="#一个简单的例子：Iris（鸢尾花）数据集" class="headerlink" title="一个简单的例子：Iris（鸢尾花）数据集"></a>一个简单的例子：Iris（鸢尾花）数据集</h2><p>在这个例子中，我们将看一下存储在sklearn中的iris数据。这个数据集包含三种不同种类的iris测量值。让我们看下下图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> IPython.core.display <span class="keyword">import</span> Image, display</div><div class="line">display(Image(filename=<span class="string">'images/iris_setosa.jpg'</span>))</div><div class="line">print(<span class="string">"Iris Setosa\n"</span>)</div><div class="line"></div><div class="line">display(Image(filename=<span class="string">'images/iris_versicolor.jpg'</span>))</div><div class="line">print(<span class="string">"Iris Versicolor\n"</span>)</div><div class="line"></div><div class="line">display(Image(filename=<span class="string">'images/iris_virginica.jpg'</span>))</div><div class="line">print(<span class="string">"Iris Virginica"</span>)</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-09-133850.jpg" alt="jpeg"></p>
<pre><code>Iris Setosa
</code></pre><p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-09-133913.jpg" alt="jpeg"></p>
<pre><code>Iris Versicolor
</code></pre><p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-09-133930.jpg" alt="jpeg"></p>
<pre><code>Iris Virginica
</code></pre><h3 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h3><p><strong>如果我们需要一个算法来识别不同种类的iris（鸢尾花），需要哪些数据？</strong></p>
<p>我们需要一个<code>[n_samples x n_features]</code>的二维数组</p>
<ul>
<li><code>n_samples</code>表示什么？</li>
<li><code>n_features</code>表示什么？</li>
</ul>
<p>每一个样本必须有确定数量的特征，每个特征都是样本的某种量化值。</p>
<h3 id="从sklearn中加载iris数据集"><a href="#从sklearn中加载iris数据集" class="headerlink" title="从sklearn中加载iris数据集"></a>从sklearn中加载iris数据集</h3><p>sklearn中包含了这些种类的iris数据集。这数据集包含下面的特征值：</p>
<ul>
<li>数据集中的特征：</li>
</ul>
<ol>
<li>花萼长度（单位cm）</li>
<li>花萼宽度（单位cm）</li>
<li>花瓣长度（单位cm）</li>
<li>花瓣宽度（单位cm）</li>
</ol>
<ul>
<li>预测的目标分类：</li>
</ul>
<ol>
<li>山鸢尾（Iris Setosa）</li>
<li>杂色鸢尾（Iris Versicolour）</li>
<li>维吉尼亚鸢尾（Iris Virginica）</li>
</ol>
<p><code>sklearn</code>包含iris的CSV文件包括加载到numpy arrays的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</div><div class="line">iris = load_iris()</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">iris.keys()</div></pre></td></tr></table></figure>
<pre><code>[&apos;target_names&apos;, &apos;data&apos;, &apos;target&apos;, &apos;DESCR&apos;, &apos;feature_names&apos;]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">n_samples, n_features = iris.data.shape</div><div class="line">print((n_samples, n_features))</div><div class="line">print(iris.data[<span class="number">0</span>])</div></pre></td></tr></table></figure>
<pre><code>(150, 4)
[ 5.1  3.5  1.4  0.2]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">print(iris.data.shape)</div><div class="line">print(iris.target.shape)</div></pre></td></tr></table></figure>
<pre><code>(150, 4)
(150,)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(iris.target)</div></pre></td></tr></table></figure>
<pre><code>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
</code></pre><p>这个数据集是四维的，但是我们可以用scatter plot每次展示两维：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">x_index = <span class="number">0</span></div><div class="line">y_index = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># this formatter will label the colorbar with the correct target names</span></div><div class="line">formatter = plt.FuncFormatter(<span class="keyword">lambda</span> i, *args: iris.target_names[int(i)])</div><div class="line"></div><div class="line">plt.scatter(iris.data[:, x_index], iris.data[:, y_index],</div><div class="line">            c=iris.target, cmap=plt.cm.get_cmap(<span class="string">'RdYlBu'</span>, <span class="number">3</span>))</div><div class="line">plt.colorbar(ticks=[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], format=formatter)</div><div class="line">plt.clim(<span class="number">-0.5</span>, <span class="number">2.5</span>)</div><div class="line">plt.xlabel(iris.feature_names[x_index])</div><div class="line">plt.ylabel(iris.feature_names[y_index]);</div></pre></td></tr></table></figure>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-09-133955.jpg" alt="png"></p>
<h3 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h3><p>改变上面代码中<code>x_index</code>和<code>y_index</code>值，找出一个点可以最大化的分割这三类</p>
<p>这是个<strong>降维</strong>的练习，我们稍后会见到。</p>
<h2 id="其他可用的数据"><a href="#其他可用的数据" class="headerlink" title="其他可用的数据"></a>其他可用的数据</h2><p>有以下的三种形式：</p>
<ul>
<li><strong>sklearn包数据</strong> 在安装的时候，这些小的数据集被打包在sklearn中，通过<code>sklearn.datasets.load_*</code>函数能够被加载</li>
<li><strong>下载的数据</strong> 这些大的数据集可以通过sklearn提供的<code>sklearn.datasets.fetch_*</code>函数从网上下载</li>
<li><strong>生成的数据</strong> 这些数据可以通过sklearn提供的<code>sklearn.datasets.make_*</code>函数，基于一个随机种子从模型中产生</li>
</ul>
<p>你能通过IPython的tab补全功能，浏览这些函数。在从sklearn中导入<code>datasets</code>子模块后，你可以<br>输入datasets.load<em> + TAB或datasets.fetch</em> + TAB或datasets.make_ + TAB来浏览这些函数列表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Type datasets.fetch_&lt;TAB&gt; or datasets.load_&lt;TAB&gt; in IPython to see all possibilities</span></div><div class="line"></div><div class="line"><span class="comment"># datasets.fetch_</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># datasets.load_</span></div></pre></td></tr></table></figure>
<p>在下一章，我们将使用这些数据集，并且学习机器学习的基本方法。</p>
<hr>
<p><a href="https://github.com/yummybian/sklearn_pycon2015_zh.git" target="_blank" rel="external">Jupyter实现</a></p>
<hr>
<p><a rel="external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/3.0/88x31.png"></a><br>本作品采用<a rel="external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank">知识共享署名-非商业性使用-禁止演绎 3.0 未本地化版本许可协议</a>进行许可。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/09/在k8s中如何使用GPU/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="彬叔">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bin's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/09/在k8s中如何使用GPU/" itemprop="url">在k8s中如何使用GPU</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-09T17:09:15+08:00">
                2017-11-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/微服务/" itemprop="url" rel="index">
                    <span itemprop="name">微服务</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/11/09/在k8s中如何使用GPU/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/11/09/在k8s中如何使用GPU/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/11/09/在k8s中如何使用GPU/" class="leancloud_visitors" data-flag-title="在k8s中如何使用GPU">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>部署<strong>All In One</strong> K8s，可以参考这位老兄<a href="http://stuartingram.com/2016/08/31/installing-kubernetes-on-ubuntu-14-04/" target="_blank" rel="external">文章</a>。</p>
<p><strong>此文章只适用于nvidia-docker 1.0</strong></p>
<h2 id="准备工作："><a href="#准备工作：" class="headerlink" title="准备工作："></a>准备工作：</h2><ul>
<li>安装NVIDIA驱动，安装完成后运行<code>nvidia-docker-plugin</code>来确认驱动是否被正确的加载。</li>
<li>查看nvidia device信息<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -s localhost:3476/docker/cli</div></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="不通过nvidia-docker启动GPU容器"><a href="#不通过nvidia-docker启动GPU容器" class="headerlink" title="不通过nvidia-docker启动GPU容器"></a>不通过nvidia-docker启动GPU容器</h2><h3 id="手动指定驱动路径，然后挂载驱动和CUDA库"><a href="#手动指定驱动路径，然后挂载驱动和CUDA库" class="headerlink" title="手动指定驱动路径，然后挂载驱动和CUDA库"></a>手动指定驱动路径，然后挂载驱动和CUDA库</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> DRIVER=<span class="string">'-v /var/lib/nvidia-docker/volumes/nvidia_driver/375.66:/usr/local/nvidia'</span></div><div class="line"><span class="built_in">export</span> CUDA_SO=$(\ls /usr/lib/x86_64-linux-gnu/libcuda.* | xargs -I&#123;&#125; <span class="built_in">echo</span> <span class="string">'--volume &#123;&#125;:&#123;&#125;'</span>)</div><div class="line"><span class="built_in">export</span> DEVICES=$(\ls /dev/nvidia* | xargs -I&#123;&#125; <span class="built_in">echo</span> <span class="string">'--device &#123;&#125;:&#123;&#125;'</span>)</div></pre></td></tr></table></figure>
<h3 id="进入tf容器"><a href="#进入tf容器" class="headerlink" title="进入tf容器"></a>进入tf容器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker run --rm -it <span class="variable">$DRIVER</span> <span class="variable">$CUDA_SO</span> <span class="variable">$DEVICES</span> tensorflow/tensorflow:1.2.1-devel-gpu-py3</div></pre></td></tr></table></figure>
<h3 id="在tf容器中用nvidia-smi命令查看GPU信息"><a href="#在tf容器中用nvidia-smi命令查看GPU信息" class="headerlink" title="在tf容器中用nvidia-smi命令查看GPU信息"></a>在tf容器中用<code>nvidia-smi</code>命令查看GPU信息</h3><p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-08-143028.png" alt="image-02"><br>可以看到两块GPU都已经正常映射</p>
<h3 id="在tf容器中运行下面的代码，看tf是否能正常工作"><a href="#在tf容器中运行下面的代码，看tf是否能正常工作" class="headerlink" title="在tf容器中运行下面的代码，看tf是否能正常工作"></a>在tf容器中运行下面的代码，看tf是否能正常工作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="comment"># Creates a graph.</span></div><div class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'a'</span>)</div><div class="line">b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">3</span>, <span class="number">2</span>], name=<span class="string">'b'</span>)</div><div class="line">c = tf.matmul(a, b)</div><div class="line"><span class="comment"># Creates a session with log_device_placement set to True.</span></div><div class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=<span class="keyword">True</span>))</div><div class="line"><span class="comment"># Runs the op.</span></div><div class="line">print(sess.run(c))</div></pre></td></tr></table></figure>
<h2 id="在k8s中使用GPU"><a href="#在k8s中使用GPU" class="headerlink" title="在k8s中使用GPU"></a>在k8s中使用GPU</h2><ul>
<li><p>由于支持GPU是一个试验性的feature，故我们需要修改一下启动参数，在hack/local-up-cluster.sh中修改</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">FEATURE_GATES=<span class="variable">$&#123;FEATURE_GATES:-"Accelerators=true"&#125;</span></div></pre></td></tr></table></figure>
</li>
<li><p>查看node所能提供的资源上限</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kubectl get nodes -o json | jq <span class="string">'.items[] | &#123;name: .metadata.name, cap: .status.capacity&#125;'</span></div></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="http://oyows3qd4.bkt.clouddn.com/blog/2017-11-08-140659.png" alt="image-02"><br>看到有两块GPU</p>
<ul>
<li>编写pod spec<blockquote>
<p>有两个地方需要注意</p>
</blockquote>
</li>
</ul>
<ol>
<li>挂载nvidia驱动和CUDA库。</li>
<li>增加<code>&quot;alpha.kubernetes.io/nvidia-gpu&quot;: &quot;1&quot;</code>，表示申请1块GPU。<br><a href="https://gist.github.com/yummybian/60f185875a43778b8835468ec99ea3f3" target="_blank" rel="external">完整例子</a></li>
</ol>
<p>参考：<br><a href="https://medium.com/jim-fleming/running-tensorflow-on-kubernetes-ca00d0e67539" target="_blank" rel="external">Running TensorFlow (with GPU) on Kubernetes</a></p>
<p><a rel="external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/3.0/88x31.png"></a><br>本作品采用<a rel="external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank">知识共享署名-非商业性使用-禁止演绎 3.0 未本地化版本许可协议</a>进行许可。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/04/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="彬叔">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bin's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/04/hello-world/" itemprop="url">Hello World</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-04T17:26:45+08:00">
                2017-11-04
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/11/04/hello-world/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/11/04/hello-world/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/11/04/hello-world/" class="leancloud_visitors" data-flag-title="Hello World">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">彬叔</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/yummybian" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:yummybian@gmail.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.douban.com/people/3280249/" target="_blank" title="豆瓣">
                    
                      <i class="fa fa-fw fa-globe"></i>豆瓣</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/binbin.io" target="_blank" title="知乎">
                    
                      <i class="fa fa-fw fa-globe"></i>知乎</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">彬叔</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  

    
      <script id="dsq-count-scr" src="https://bin.disqus.com/count.js" async></script>
    

    

  




	





  











<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="//unpkg.com/valine/dist/Valine.min.js"></script>



  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("riExM5iqp9SpB8bmWrbcTJz6-gzGzoHsz", "zl9Exu2Rqq7DOkCvxmRsuzjC");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  

  

  

</body>
</html>
